{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_train.csv', index_col=0)\n",
    "test1_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test1.csv', index_col=0)\n",
    "test2_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test2.csv', index_col=0)\n",
    "test3_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'bedrooms', 'year_built', 'bathrooms', 'min_to_subway', 'size_sqft', 'no_fee', 'addr_zip', 'floornumber', 'floor_count', \n",
    "    'has_doorman', 'has_fireplace', 'has_gym', 'allows_pets', 'has_washer_dryer', 'has_garage',\n",
    "    'has_roofdeck', 'has_concierge', 'has_pool', 'has_garden'\n",
    "\n",
    "]\n",
    "\n",
    "train_features = train_df[feature_cols] \n",
    "\n",
    "# impute missing values with medians\n",
    "train_features = train_features.fillna(train_features.median(), axis=0)\n",
    "\n",
    "# construct target vector\n",
    "train_target = train_df['rent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Training Data Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_imputation(df, feature):\n",
    "\n",
    "    number_missing = df[feature].isnull().sum()\n",
    "    observed_values = df.loc[df[feature].notnull(), feature]\n",
    "    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)    \n",
    "    return df\n",
    "\n",
    "#missing_columns = [\"year_built\", \"min_to_subway\", \"floornumber\"]\n",
    "#for feature in missing_columns:\n",
    "#    test1_features[feature + '_imp'] = [feature]\n",
    "#    test1_features = random_imputation(test1_features, feature)\n",
    "\n",
    "test1_features = test1_df[feature_cols] \n",
    "test1_features = test1_features.fillna(train_features.median(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=green> Test 1 <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3297096.0901556876"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LinearRegression()\n",
    "lreg.fit(train_features, train_target)\n",
    "\n",
    "test1_df['predicted'] = lreg.predict(test1_features)\n",
    "mean_squared_error(test1_df['rent'], test1_df['predicted'])\n",
    "#test_df['predicted'].to_csv('linear.csv', header=True)\n",
    "#3301338.418329301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746490.3228272318"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 50, random_state = 1, max_depth=42)\n",
    "regressor.fit(train_features, train_target)\n",
    "\n",
    "test1_df['predicted'] = regressor.predict(test1_features)\n",
    "mean_squared_error(test1_df['rent'], test1_df['predicted'])\n",
    "#test_df['predicted'].to_csv('linear.csv', header=True)\n",
    "#3301338.418329301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Data, Predict Values for Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=42,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df = train_df.append(test1_df, sort=False)\n",
    "\n",
    "master_features = master_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "master_target = master_df['rent']\n",
    "\n",
    "regressor.fit(master_features, master_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=green> Test 2 <font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Training Data Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_features = test1_df[feature_cols] \n",
    "test2_features = test1_features.fillna(train_features.median(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_df['predicted'] = regressor.predict(test2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_df['predicted'].to_csv('test2_results.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=green> Importing New Data <font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "dob_complaints_df = pd.read_csv('C:/Users/new/Downloads/data/DOB_Complaints_Received.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "dob_violations_df = pd.read_csv('C:/Users/new/Downloads/data/DOB_Violations.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "property_value_df = pd.read_csv('C:/Users/new/Downloads/data/Revised_Notice_of_Property_Value__RNOPV_.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dob_complaints_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dob_violations_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#property_value_df.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_value_df = property_value_df[['BIN','ORIGINAL MARKET VALUE']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#property_value_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_violations_df['ISSUE_DATE'] =  pd.to_datetime(dob_violations_df['ISSUE_DATE'], format='%Y%m%d', errors='coerce')\n",
    "dob_violations_df = dob_violations_df[(dob_violations_df['ISSUE_DATE'].dt.year > 2010) ]\n",
    "bin_violations = dob_violations_df.BIN.value_counts().reset_index().rename(columns={'index': 'z', 0: 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = train_df.append(test1_df, sort=False)\n",
    "\n",
    "#master_df = pd.merge(left=master_df,right=bin_violations, left_on='bin', right_on='z')\n",
    "master_df = pd.merge(left=master_df,right=bin_violations,  how='left', left_on='bin', right_on='z')\n",
    "master_df.rename(columns={'BIN': 'dob_violations_count'}, inplace=True)\n",
    "master_df.drop('z', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df = pd.merge(left=master_df,right=property_value_df, left_on='bin', right_on='ORIGINAL MARKET VALUE')\n",
    "master_df = pd.merge(left=master_df,right=property_value_df,  how='left', left_on='bin', right_on='ORIGINAL MARKET VALUE')\n",
    "master_df.rename(columns={'ORIGINAL MARKET VALUE': 'market_value'}, inplace=True)\n",
    "\n",
    "master_df.drop('BIN', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_complaints_df['Date Entered'] =  pd.to_datetime(dob_complaints_df['Date Entered'], format='%m/%d/%Y')\n",
    "dob_complaints_df  = dob_complaints_df[(dob_complaints_df['Date Entered'].dt.year > 2010) ]\n",
    "bin_complaints = dob_complaints_df.BIN.value_counts().reset_index().rename(columns={'index': 'z', 0: 'count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df = pd.merge(left=master_df,right=bin_complaints, left_on='bin', right_on='z')\n",
    "master_df = pd.merge(left=master_df,right=bin_complaints,  how='left', left_on='bin', right_on='z')\n",
    "master_df.rename(columns={'BIN': 'dob_complaints_count'}, inplace=True)\n",
    "master_df.drop('z', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df = pd.read_csv('C:/Users/new/Downloads/data/Property_Valuation_and_Assessment_data_Tax_Classes_2_3_4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
